import gc
import logging
import torch

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

model_status = {"sdxl": False, "ollama": False, "multimodal": False}
latest_generated_image = None


def clear_cuda_memory():
    """Clear CUDA cache and run garbage collection."""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()
    logger.info("CUDA memory cleared and garbage collection performed")


def get_model_status():
    """Return formatted Markdown describing current model availability."""
    status_text = "ü§ñ **Model Status:**\n"
    status_text += f"‚Ä¢ SDXL: {'‚úÖ Loaded' if model_status['sdxl'] else '‚ùå Not loaded'}\n"
    status_text += f"‚Ä¢ Ollama: {'‚úÖ Connected' if model_status['ollama'] else '‚ùå Not connected'}\n"
    status_text += f"‚Ä¢ Vision: {'‚úÖ Available' if model_status['multimodal'] else '‚ùå Not available'}\n"
    status_text += f"‚Ä¢ CUDA: {'‚úÖ Available' if torch.cuda.is_available() else '‚ùå Not available'}"
    return status_text
