import gc
import logging
import torch

from .state import AppState

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


def clear_cuda_memory():
    """Clear CUDA cache and run garbage collection."""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()
    logger.info("CUDA memory cleared and garbage collection performed")


def get_model_status(state: AppState) -> str:
    """Return formatted Markdown describing current model availability."""
    status_text = "ğŸ¤– **Model Status:**\n"
    status_text += f"â€¢ SDXL: {'âœ… Loaded' if state.model_status['sdxl'] else 'âŒ Not loaded'}\n"
    status_text += f"â€¢ Ollama: {'âœ… Connected' if state.model_status['ollama'] else 'âŒ Not connected'}\n"
    status_text += f"â€¢ Vision: {'âœ… Available' if state.model_status['multimodal'] else 'âŒ Not available'}\n"
    status_text += f"â€¢ CUDA: {'âœ… Available' if torch.cuda.is_available() else 'âŒ Not available'}"
    return status_text
