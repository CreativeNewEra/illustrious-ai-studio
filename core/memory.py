import gc
import logging
import torch

from .state import AppState
from .config import CONFIG

logger = logging.getLogger(__name__)


def clear_gpu_memory():
    """Clear GPU cache and run garbage collection based on backend."""
    if CONFIG.gpu_backend in ("cuda", "rocm") and torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()
    logger.info("GPU memory cleared and garbage collection performed")


def get_model_status(state: AppState) -> str:
    """Return formatted Markdown describing current model availability."""
    status_text = "ğŸ¤– **Model Status:**\n"
    status_text += f"â€¢ SDXL: {'âœ… Loaded' if state.model_status['sdxl'] else 'âŒ Not loaded'}\n"
    status_text += f"â€¢ Ollama: {'âœ… Connected' if state.model_status['ollama'] else 'âŒ Not connected'}\n"
    status_text += f"â€¢ Vision: {'âœ… Available' if state.model_status['multimodal'] else 'âŒ Not available'}\n"
    status_text += f"â€¢ Backend: {CONFIG.gpu_backend}\n"
    status_text += f"â€¢ GPU: {'âœ… Available' if torch.cuda.is_available() else 'âŒ Not available'}"
    return status_text
